{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duyxbv2OLPGF"
      },
      "source": [
        "https://dashboard.ngrok.com/get-started/your-authtoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPwKRtKILPbC",
        "outputId": "2acc35d8-ba5f-4c0b-c8e5-379089857ba0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.45.1-py3-none-any.whl.metadata (8.9 kB)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.1.8)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (24.2)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.2.1)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.4)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (9.1.2)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.13.2)\n",
            "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.38.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.4.26)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.24.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Downloading streamlit-1.45.1-py3-none-any.whl (9.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyngrok-7.2.8-py3-none-any.whl (25 kB)\n",
            "Downloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: watchdog, pyngrok, pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 pyngrok-7.2.8 streamlit-1.45.1 watchdog-6.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install streamlit pyngrok\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rTAC0fHYLXtA"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# T·∫°o c·∫•u tr√∫c th∆∞ m·ª•c\n",
        "os.makedirs(\"streamlit_app/tabs\", exist_ok=True)\n",
        "\n",
        "# T·∫°o __init__.py ƒë·ªÉ Python nh·∫≠n bi·∫øt ƒë√¢y l√† module\n",
        "with open(\"streamlit_app/tabs/__init__.py\", \"w\") as f:\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nSoqZwBMlei"
      },
      "source": [
        "## schema_tab.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sg06kf2sMCKA",
        "outputId": "5681958a-7f1f-44dc-a621-7a07f4716f3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting streamlit_app/tabs/schema_tab.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile streamlit_app/tabs/schema_tab.py\n",
        "import streamlit as st\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "def show_schema(df):\n",
        "    # Reading data\n",
        "    st.subheader(\"üìã Dataset\")\n",
        "    st.dataframe(df.limit(100).toPandas())\n",
        "\n",
        "    # Data Schema\n",
        "    st.subheader(\"üß¨ Data Schema\")\n",
        "    st.code(df._jdf.schema().treeString())\n",
        "\n",
        "    # count the number of rows and columns in the dataset\n",
        "    st.subheader(\"üìä Data Dimensions\")\n",
        "    st.write(f\"Rows: {df.count()}\")\n",
        "    st.write(f\"Columns: {len(df.columns)}\")\n",
        "\n",
        "    # Check duplicate\n",
        "    st.subheader(\"üîç Check duplicate\")\n",
        "    distinct_rows = df.distinct().count()\n",
        "    duplicates = df.count() - distinct_rows\n",
        "\n",
        "    if duplicates > 0:\n",
        "        st.warning(f\"üö® Number of duplicate rows: `{duplicates}`\")\n",
        "    else:\n",
        "        st.success(\"‚úÖ No duplicate rows found.\")\n",
        "\n",
        "    # Define missing value\n",
        "    st.subheader(\"üîç Check missing values\")\n",
        "    # T√≠nh s·ªë l∆∞·ª£ng missing (null ho·∫∑c NaN) cho t·ª´ng c·ªôt\n",
        "    null_counts = df.select([\n",
        "        F.count(F.when(F.col(c).isNull() | F.isnan(c), c)).alias(c)\n",
        "        for c in df.columns\n",
        "    ])\n",
        "\n",
        "    # Chuy·ªÉn sang Pandas ƒë·ªÉ hi·ªÉn th·ªã trong Streamlit\n",
        "    null_df = null_counts.toPandas().T\n",
        "    null_df.columns = ['Missing Count']\n",
        "    null_df = null_df[null_df['Missing Count'] > 0]\n",
        "\n",
        "    if null_df.empty:\n",
        "        st.success(\"‚úÖ No missing values found.\")\n",
        "    else:\n",
        "        st.dataframe(null_df)\n",
        "\n",
        "    # Data Reduction\n",
        "    st.subheader(\"üîç Data Reduction\")\n",
        "    if 'order_id' in df.columns:\n",
        "        df = df.drop(\"order_id\")\n",
        "        st.success(\"'order_id' column removed.\")\n",
        "    else:\n",
        "        st.info(\"'order_id' column not found in dataset.\")\n",
        "\n",
        "    # Data Cleaning/Wrangling\n",
        "    st.subheader(\"üîç Data Cleaning/Wrangling\")\n",
        "    before_drop = df.count()\n",
        "    df = df.dropna(subset=['transaction_type'])\n",
        "    after_drop = df.count()\n",
        "\n",
        "    st.write(f\"Rows before drop: `{before_drop}`\")\n",
        "    st.write(f\"Rows after drop: `{after_drop}`\")\n",
        "    st.success(f\"‚úÖ Dropped `{before_drop - after_drop}` rows with null in 'transaction_type'.\")\n",
        "\n",
        "    # Show data after drop NAN\n",
        "    st.subheader(\"üìã Dataset after drop Null\")\n",
        "    st.dataframe(df.limit(100).toPandas())\n",
        "\n",
        "    # Statistics Summary\n",
        "    st.subheader(\"üìä Statistics Summary\")\n",
        "    # Chuy·ªÉn k·∫øt qu·∫£ t·ª´ PySpark describe() sang Pandas\n",
        "    stats_summary = df.describe().toPandas()\n",
        "    st.dataframe(stats_summary)\n",
        "\n",
        "    # Cu·ªëi h√†m show_schema\n",
        "    st.session_state.df = df  # üëà L∆∞u df1 v√†o session_state\n",
        "    return df\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIDmZZRsMnlc"
      },
      "source": [
        "## univariate_analysis.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LsCwEMGeAE2Y",
        "outputId": "dad05377-2331-41c3-9dba-aeb045fbe5b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting streamlit_app/tabs/univariate_analysis.py\n"
          ]
        }
      ],
      "source": [
        "# EDA Univariate Analysis\n",
        "%%writefile streamlit_app/tabs/univariate_analysis.py\n",
        "import streamlit as st\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "def calculate_percentage(df, column_name):\n",
        "    total_count = df.count()\n",
        "    percentage_df = (\n",
        "        df.groupBy(column_name)\n",
        "          .agg(F.count(\"*\").alias(\"count\"))\n",
        "          .withColumn(\"percentage\", F.round((F.col(\"count\") / total_count) * 100, 2))\n",
        "          .orderBy(F.col(\"count\").desc())\n",
        "    )\n",
        "    return percentage_df\n",
        "\n",
        "def univariate_analysis(df):\n",
        "    # st.subheader(\"üìä Univariate Analysis\")\n",
        "\n",
        "    if 'df' not in st.session_state:\n",
        "        st.error(\"‚ùå Data is not available. Please run the 'Schema' tab first.\")\n",
        "        return\n",
        "\n",
        "    df = st.session_state.df  # üëà L·∫•y df t·ª´ session_state\n",
        "\n",
        "    st.subheader(\"üìã Dataset\")\n",
        "    st.dataframe(df.limit(5).toPandas())\n",
        "    st.write(f\"Rows: {df.count()}\")\n",
        "    st.write(f\"Columns: {len(df.columns)}\")\n",
        "\n",
        "    numeric_cols = [\"item_price\", \"quantity\", \"transaction_amount\"]\n",
        "    categorical_cols = [\"item_name\", \"item_type\", \"transaction_type\", \"received_by\", \"time_of_sale\"]\n",
        "\n",
        "    # Convert small sample from Spark to Pandas\n",
        "    pandas_df = df.sample(withReplacement=False, fraction=0.1, seed=1).toPandas()\n",
        "\n",
        "    # ----- 1. Numeric Columns -----\n",
        "    st.markdown(\"### üî¢ Numeric Columns\")\n",
        "    for col in numeric_cols:\n",
        "        st.markdown(f\"#### ‚û§ `{col}`\")\n",
        "        skewness = round(pandas_df[col].skew(), 2)\n",
        "        st.write(f\"Skewness: `{skewness}`\")\n",
        "\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "        # Histogram\n",
        "        sns.histplot(pandas_df[col].dropna(), kde=True, bins=30, ax=axes[0])\n",
        "        axes[0].set_title(f\"Histogram of {col}\")\n",
        "        axes[0].grid(True)\n",
        "\n",
        "        # Boxplot\n",
        "        sns.boxplot(x=pandas_df[col].dropna(), color='orange', ax=axes[1])\n",
        "        axes[1].set_title(f\"Boxplot of {col}\")\n",
        "        axes[1].grid(True)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        st.pyplot(fig)\n",
        "\n",
        "    # ----- 2. Categorical Columns -----\n",
        "    st.markdown(\"### üè∑Ô∏è Categorical Columns\")\n",
        "\n",
        "    for col in categorical_cols:\n",
        "        st.markdown(f\"#### ‚û§ `{col}`\")\n",
        "\n",
        "        # T√≠nh t·ª∑ l·ªá ph·∫ßn trƒÉm b·∫±ng Spark\n",
        "        pct_df = calculate_percentage(df, col)\n",
        "        pandas_pct = pct_df.toPandas()\n",
        "\n",
        "        # Hi·ªÉn th·ªã b·∫£ng\n",
        "        st.dataframe(pandas_pct)\n",
        "\n",
        "        # Pie chart\n",
        "        labels = pandas_pct[col].tolist()\n",
        "        sizes = pandas_pct[\"percentage\"].tolist()\n",
        "        colors = plt.cm.Pastel1(np.linspace(0, 1, len(sizes)))\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(5, 5))\n",
        "        wedges, texts, autotexts = ax.pie(\n",
        "            sizes,\n",
        "            labels=labels,\n",
        "            autopct='%1.1f%%',\n",
        "            startangle=90,\n",
        "            colors=colors,\n",
        "            textprops={'fontsize': 9}\n",
        "        )\n",
        "        ax.axis('equal')\n",
        "        ax.set_title(f\"Distribution of {col}\")\n",
        "\n",
        "        st.pyplot(fig)\n",
        "\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9aRO_5nMp8n"
      },
      "source": [
        "## data_transformation.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHDlo1PCMFYY",
        "outputId": "904adcc4-89a5-4ec8-f5b4-fe21615cb7c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting streamlit_app/tabs/data_transformation.py\n"
          ]
        }
      ],
      "source": [
        "# Data Transformation\n",
        "%%writefile streamlit_app/tabs/data_transformation.py\n",
        "import streamlit as st\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql import functions as F\n",
        "from functools import reduce\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def data_transformation(df):\n",
        "    if 'df' not in st.session_state:\n",
        "        st.error(\"‚ùå Data is not available. Please run the 'Schema' tab first.\")\n",
        "        return\n",
        "\n",
        "    df = st.session_state.df  # üëà L·∫•y df t·ª´ session_state\n",
        "\n",
        "    st.subheader(\"üìã Dataset\")\n",
        "    st.dataframe(df.limit(5).toPandas())\n",
        "    st.write(f\"Rows: {df.count()}\")\n",
        "    st.write(f\"Columns: {len(df.columns)}\")\n",
        "\n",
        "    # st.header(\"üìê Data Transformation\")\n",
        "\n",
        "    numeric_cols = [\"item_price\", \"quantity\", \"transaction_amount\"]\n",
        "    outlier_info = {}\n",
        "\n",
        "    # T√≠nh to√°n Q1, Q3, IQR v√† Outliers\n",
        "    for col_name in numeric_cols:\n",
        "        quantiles = df.approxQuantile(col_name, [0.25, 0.75], 0.05)\n",
        "        if len(quantiles) < 2:\n",
        "            continue\n",
        "        Q1, Q3 = quantiles\n",
        "        IQR = Q3 - Q1\n",
        "\n",
        "        lower_bound = Q1 - 1.5 * IQR\n",
        "        upper_bound = Q3 + 1.5 * IQR\n",
        "        outliers = df.filter((col(col_name) < lower_bound) | (col(col_name) > upper_bound))\n",
        "        outlier_count = outliers.count()\n",
        "\n",
        "        outlier_info[col_name] = {\n",
        "            \"Q1\": Q1, \"Q3\": Q3, \"IQR\": IQR,\n",
        "            \"Lower Bound\": lower_bound,\n",
        "            \"Upper Bound\": upper_bound,\n",
        "            \"Outlier Count\": outlier_count,\n",
        "            \"Outlier Data\": outliers\n",
        "        }\n",
        "\n",
        "    # Hi·ªÉn th·ªã th√¥ng tin outlier\n",
        "    for col_name, stats in outlier_info.items():\n",
        "        st.subheader(f\"üìä Outlier Info - {col_name}\")\n",
        "        st.markdown(f\"\"\"\n",
        "        - **Q1**: {stats['Q1']}\n",
        "        - **Q3**: {stats['Q3']}\n",
        "        - **IQR**: {stats['IQR']}\n",
        "        - **Lower Bound**: {stats['Lower Bound']}\n",
        "        - **Upper Bound**: {stats['Upper Bound']}\n",
        "        - **Outlier Count**: {stats['Outlier Count']}\n",
        "        \"\"\")\n",
        "        st.write(f\"üîç Outliers for column `{col_name}`\")\n",
        "        st.dataframe(stats[\"Outlier Data\"].toPandas(), use_container_width=True)\n",
        "\n",
        "    # Drop outliers\n",
        "    filter_conditions = []\n",
        "    for col_name in numeric_cols:\n",
        "        stats = outlier_info[col_name]\n",
        "        lower = stats[\"Lower Bound\"]\n",
        "        upper = stats[\"Upper Bound\"]\n",
        "        condition = (col(col_name) >= lower) & (col(col_name) <= upper)\n",
        "        filter_conditions.append(condition)\n",
        "\n",
        "    combined_condition = reduce(lambda x, y: x & y, filter_conditions)\n",
        "    df = df.filter(combined_condition)\n",
        "\n",
        "    st.markdown(\"### üìã Data After Removing Outliers\")\n",
        "    st.dataframe(df.limit(100).toPandas())\n",
        "    st.success(f\"Original row count: {df.count()}\")\n",
        "    st.success(f\"Row count after removing outliers: {df.count()}\")\n",
        "\n",
        "    # L·∫•y m·∫´u v√† chuy·ªÉn sang Pandas\n",
        "    pandas_df = df.sample(False, 0.1, seed=1).toPandas()\n",
        "\n",
        "    # Plot histogram + boxplot\n",
        "    for col_name in numeric_cols:\n",
        "        st.markdown(f\"### üìà Distribution for `{col_name}`\")\n",
        "        st.write(\"Skewness:\", round(pandas_df[col_name].skew(), 2))\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "        sns.histplot(pandas_df[col_name].dropna(), kde=True, ax=axes[0])\n",
        "        axes[0].set_title(f\"Histogram of {col_name}\")\n",
        "\n",
        "        sns.boxplot(x=pandas_df[col_name].dropna(), color='orange', ax=axes[1])\n",
        "        axes[1].set_title(f\"Boxplot of {col_name}\")\n",
        "\n",
        "        st.pyplot(fig)\n",
        "\n",
        "    st.session_state.df = df  # üëà L∆∞u df1 v√†o session_state\n",
        "\n",
        "    st.markdown(\"### üìã Data Summary\")\n",
        "    st.dataframe(df.describe().toPandas())\n",
        "\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbKSFZH3Mr5W"
      },
      "source": [
        "## bivariate_analysis.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SEe9KBSOMHhK",
        "outputId": "439df591-87c6-4756-8c41-44030c06d1e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting streamlit_app/tabs/bivariate_analysis.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile streamlit_app/tabs/bivariate_analysis.py\n",
        "import streamlit as st\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def bivariate_analysis(df):\n",
        "    # T·∫°o tab EDA Bivariate Analysis\n",
        "    if 'df' not in st.session_state:\n",
        "        st.error(\"‚ùå Data is not available. Please run the 'Schema' tab first.\")\n",
        "        return\n",
        "\n",
        "    df = st.session_state.df  # üëà L·∫•y df t·ª´ session_state\n",
        "\n",
        "    st.subheader(\"üìã Dataset\")\n",
        "    st.dataframe(df.limit(5).toPandas())\n",
        "    st.write(f\"Rows: {df.count()}\")\n",
        "    st.write(f\"Columns: {len(df.columns)}\")\n",
        "\n",
        "    # st.header(\"üìä EDA Bivariate Analysis\")\n",
        "\n",
        "    # Ch·ªçn c√°c c·ªôt ki·ªÉu s·ªë v√† chuy·ªÉn t·ª´ PySpark DataFrame sang Pandas\n",
        "    numeric_columns = ['quantity', 'item_price', 'transaction_amount']\n",
        "    pandas_df = df.select(numeric_columns).toPandas()\n",
        "\n",
        "    # V·∫Ω bi·ªÉu ƒë·ªì Pairplot ƒë·ªÉ kh√°m ph√° m·ªëi quan h·ªá gi·ªØa c√°c bi·∫øn\n",
        "    st.subheader(\"Pairplot of Numerical Columns\")\n",
        "    pairplot = sns.pairplot(pandas_df)\n",
        "    st.pyplot(pairplot.figure)  # ‚úÖ Pass figure explicitly to avoid deprecation warning\n",
        "\n",
        "    # T√≠nh trung b√¨nh 'transaction_amount' theo t·ª´ng nh√≥m\n",
        "    item_type_avg = df.groupBy('item_type').avg('transaction_amount').orderBy('avg(transaction_amount)', ascending=False).toPandas()\n",
        "    transaction_type_avg = df.groupBy('transaction_type').avg('transaction_amount').orderBy('avg(transaction_amount)', ascending=False).toPandas()\n",
        "    item_name_avg = df.groupBy('item_name').avg('transaction_amount').orderBy('avg(transaction_amount)', ascending=False).limit(10).toPandas()\n",
        "    received_by_avg = df.groupBy('received_by').avg('transaction_amount').orderBy('avg(transaction_amount)', ascending=False).toPandas()\n",
        "    time_of_sale_avg = df.groupBy('time_of_sale').avg('transaction_amount').orderBy('avg(transaction_amount)', ascending=False).limit(10).toPandas()\n",
        "    quantity_avg = df.groupBy('quantity').avg('transaction_amount').orderBy('avg(transaction_amount)', ascending=False).toPandas()\n",
        "\n",
        "    # V·∫Ω bi·ªÉu ƒë·ªì\n",
        "    st.subheader(\"Bar Plots: Average Transaction Amount by Categories\")\n",
        "    fig, axarr = plt.subplots(3, 2, figsize=(16, 20))\n",
        "\n",
        "    item_type_avg.plot.bar(x='item_type', y='avg(transaction_amount)', ax=axarr[0][0], fontsize=12)\n",
        "    axarr[0][0].set_title(\"Item Type Vs Transaction Amount\", fontsize=18)\n",
        "\n",
        "    transaction_type_avg.plot.bar(x='transaction_type', y='avg(transaction_amount)', ax=axarr[0][1], fontsize=12)\n",
        "    axarr[0][1].set_title(\"Transaction Type Vs Transaction Amount\", fontsize=18)\n",
        "\n",
        "    item_name_avg.plot.bar(x='item_name', y='avg(transaction_amount)', ax=axarr[1][0], fontsize=12)\n",
        "    axarr[1][0].set_title(\"Item Name Vs Transaction Amount\", fontsize=18)\n",
        "\n",
        "    received_by_avg.plot.bar(x='received_by', y='avg(transaction_amount)', ax=axarr[1][1], fontsize=12)\n",
        "    axarr[1][1].set_title(\"Received By Vs Transaction Amount\", fontsize=18)\n",
        "\n",
        "    time_of_sale_avg.plot.bar(x='time_of_sale', y='avg(transaction_amount)', ax=axarr[2][0], fontsize=12)\n",
        "    axarr[2][0].set_title(\"Time of Sale Vs Transaction Amount\", fontsize=18)\n",
        "\n",
        "    quantity_avg.plot.bar(x='quantity', y='avg(transaction_amount)', ax=axarr[2][1], fontsize=12)\n",
        "    axarr[2][1].set_title(\"Quantity Vs Transaction Amount\", fontsize=18)\n",
        "\n",
        "    # T√πy ch·ªânh layout\n",
        "    plt.subplots_adjust(hspace=1.0, wspace=.5)\n",
        "    sns.despine()\n",
        "\n",
        "    # Hi·ªÉn th·ªã bi·ªÉu ƒë·ªì\n",
        "    st.pyplot(fig)\n",
        "\n",
        "    # Ph√¢n t√≠ch ƒëa bi·∫øn - Ma tr·∫≠n t∆∞∆°ng quan\n",
        "    st.subheader(\"Correlation Matrix\")\n",
        "    pandas_df = df.toPandas()\n",
        "    numeric_df = pandas_df.select_dtypes(include=['number'])\n",
        "    corr_matrix = numeric_df.corr()\n",
        "\n",
        "    fig_corr, ax_corr = plt.subplots(figsize=(12, 10))\n",
        "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', ax=ax_corr)\n",
        "    ax_corr.set_title(\"Correlation Matrix\")\n",
        "    st.pyplot(fig_corr)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IyCmCr1TBaE"
      },
      "source": [
        "## run_model.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hGw8njZsT8ow",
        "outputId": "412b3b3d-be6b-4eb7-98a6-9c7b6710820b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting streamlit_app/tabs/run_model.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile streamlit_app/tabs/run_model.py\n",
        "import streamlit as st\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.regression import (\n",
        "    LinearRegression,\n",
        "    DecisionTreeRegressor,\n",
        "    RandomForestRegressor,\n",
        "    GBTRegressor\n",
        ")\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "def run_model(df):\n",
        "    # st.header(\"ü§ñ Run Regression Models\")\n",
        "    st.subheader(\"Model Evaluation Results\")\n",
        "\n",
        "    # Lo·∫°i b·ªè c√°c d√≤ng c√≥ null ·ªü c√°c c·ªôt c·∫ßn thi·∫øt\n",
        "    required_cols = [\"item_name\", \"item_type\", \"transaction_type\", \"time_of_sale\", \"item_price\", \"quantity\", \"transaction_amount\"]\n",
        "    df = df.dropna(subset=required_cols)\n",
        "\n",
        "    # C√°c c·ªôt d·∫°ng chu·ªói c·∫ßn m√£ h√≥a\n",
        "    categorical_cols = [\"item_name\", \"item_type\", \"transaction_type\", \"time_of_sale\"]\n",
        "    indexers = [\n",
        "        StringIndexer(inputCol=col, outputCol=col + \"_indexed\", handleInvalid=\"keep\")\n",
        "        for col in categorical_cols\n",
        "    ]\n",
        "\n",
        "    # C·ªôt ƒë·∫∑c tr∆∞ng ƒë·∫ßu v√†o cho m√¥ h√¨nh\n",
        "    feature_cols = [col + \"_indexed\" for col in categorical_cols] + [\"item_price\", \"quantity\"]\n",
        "    assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
        "\n",
        "    # Chia t·∫≠p train/test\n",
        "    train_data, test_data = df.randomSplit([0.8, 0.2], seed=124)\n",
        "\n",
        "    # C√°c m√¥ h√¨nh c·∫ßn ƒë√°nh gi√°\n",
        "    models = {\n",
        "        \"Linear Regression\": LinearRegression(featuresCol=\"features\", labelCol=\"transaction_amount\"),\n",
        "        \"Decision Tree\": DecisionTreeRegressor(featuresCol=\"features\", labelCol=\"transaction_amount\"),\n",
        "        \"Random Forest\": RandomForestRegressor(featuresCol=\"features\", labelCol=\"transaction_amount\", numTrees=10),\n",
        "        \"Gradient-Boosted Tree\": GBTRegressor(featuresCol=\"features\", labelCol=\"transaction_amount\", maxIter=10)\n",
        "    }\n",
        "\n",
        "    # H√†m ƒë√°nh gi√° m√¥ h√¨nh\n",
        "    def evaluate_model(name, model, train_data, test_data):\n",
        "        pipeline = Pipeline(stages=indexers + [assembler, model])\n",
        "        fitted_model = pipeline.fit(train_data)\n",
        "        predictions = fitted_model.transform(test_data)\n",
        "\n",
        "        metrics = [\"r2\", \"rmse\", \"mse\", \"mae\"]\n",
        "        results = {}\n",
        "\n",
        "        for metric in metrics:\n",
        "            evaluator = RegressionEvaluator(\n",
        "                labelCol=\"transaction_amount\",\n",
        "                predictionCol=\"prediction\",\n",
        "                metricName=metric\n",
        "            )\n",
        "            results[metric.upper()] = evaluator.evaluate(predictions)\n",
        "\n",
        "        return results\n",
        "\n",
        "    # Hi·ªÉn th·ªã b·∫£ng k·∫øt qu·∫£\n",
        "    results_table = []\n",
        "\n",
        "    for name, model in models.items():\n",
        "        # st.text(f\"Running {name}...\")\n",
        "        results = evaluate_model(name, model, train_data, test_data)\n",
        "        results_table.append({\n",
        "            \"Model\": name,\n",
        "            **results\n",
        "        })\n",
        "\n",
        "    st.dataframe(results_table)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spark SQL"
      ],
      "metadata": {
        "id": "83ZbwvLAVlhi"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQJP8u6TMuO1"
      },
      "source": [
        "## streamlit_app/app.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9js0geKdJx6h",
        "outputId": "77d703a7-1a17-46cc-8d2a-d4098774d937"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting streamlit_app/app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile streamlit_app/app.py\n",
        "import streamlit as st\n",
        "from pyspark.sql import SparkSession\n",
        "from tabs.schema_tab import show_schema\n",
        "from tabs.univariate_analysis import univariate_analysis\n",
        "from tabs.bivariate_analysis import bivariate_analysis\n",
        "from tabs.data_transformation import data_transformation\n",
        "from tabs.run_model import run_model\n",
        "\n",
        "# Kh·ªüi t·∫°o Spark Session\n",
        "spark = SparkSession.builder.appName(\"Streamlit + Spark\").getOrCreate()\n",
        "\n",
        "# Ti√™u ƒë·ªÅ cho ·ª©ng d·ª•ng\n",
        "st.title(\"üìä Big Data Dashboard with PySpark\")\n",
        "\n",
        "# T·∫£i l√™n t·ªáp CSV\n",
        "uploaded_file = st.file_uploader(\"üìÅ Upload CSV\", type=\"csv\")\n",
        "\n",
        "# Ki·ªÉm tra n·∫øu ng∆∞·ªùi d√πng t·∫£i l√™n t·ªáp\n",
        "if uploaded_file:\n",
        "    with open(\"streamlit_app/temp.csv\", \"wb\") as f:\n",
        "        f.write(uploaded_file.getbuffer())\n",
        "\n",
        "    # ƒê·ªçc d·ªØ li·ªáu CSV v√†o DataFrame PySpark\n",
        "    df = spark.read.csv(\"streamlit_app/temp.csv\", header=True, inferSchema=True)\n",
        "\n",
        "    # T·∫°o c√°c tab cho ·ª©ng d·ª•ng Streamlit\n",
        "    tab1, tab2, tab3, tab4, tab5 = st.tabs([\n",
        "        \"üìë Read data\",\n",
        "        \"üìä Univariate Analysis\",\n",
        "        \"üîÑ Data Transformation\",\n",
        "        \"üìä EDA Bivariate Analysis\",\n",
        "        \"ü§ñ Run Models\"\n",
        "    ])\n",
        "\n",
        "    # Tab Schema\n",
        "    with tab1:\n",
        "        show_schema(df)\n",
        "\n",
        "    # Tab Univariate Analysis\n",
        "    with tab2:\n",
        "        df = univariate_analysis(df)\n",
        "\n",
        "    # Tab Data Transformation\n",
        "    with tab3:\n",
        "        cleaned_df = data_transformation(df)  # üëâ Tr·∫£ v·ªÅ DataFrame ƒë√£ d·ªçn s·∫°ch\n",
        "\n",
        "    # Tab EDA Bivariate Analysis\n",
        "    with tab4:\n",
        "        # N·∫øu tab tr∆∞·ªõc ch∆∞a ch·∫°y, fallback v·ªÅ g·ªëc\n",
        "        try:\n",
        "            bivariate_analysis(cleaned_df)\n",
        "        except NameError:\n",
        "            st.warning(\"‚ö†Ô∏è Please run 'Data Transformation' tab first.\")\n",
        "            bivariate_analysis(df)\n",
        "\n",
        "    # Tab Run Model\n",
        "    with tab5:\n",
        "        run_model(cleaned_df)  # üëâ S·ª≠ d·ª•ng DataFrame ƒë√£ d·ªçn s·∫°ch\n",
        "\n",
        "else:\n",
        "    st.info(\"‚¨ÜÔ∏è Upload a CSV to get started.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1jqo4b_MNmz",
        "outputId": "597d15b0-e6bf-490c-e574-20a6dbf91d0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "!ngrok config add-authtoken 2wkyIx5eiRJjyCorlDMKDYBipTC_3eQR8H83AJEt2KsRBqHJb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZExANhMMVPz",
        "outputId": "45eb3ebb-3bc4-48f2-df17-7baff1c80d50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8503\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8503\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.23.22.199:8503\u001b[0m\n",
            "\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "import threading\n",
        "import time\n",
        "\n",
        "def run():\n",
        "    !streamlit run streamlit_app/app.py\n",
        "\n",
        "# Ch·∫°y Streamlit trong thread song song\n",
        "threading.Thread(target=run).start()\n",
        "\n",
        "# Ch·ªù v√†i gi√¢y ƒë·ªÉ Streamlit kh·ªüi ƒë·ªông\n",
        "time.sleep(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zW56UJ_0MxIn",
        "outputId": "be76f1c8-388c-4240-9370-d053ad01b64e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üåê Your Streamlit app is live at: NgrokTunnel: \"https://9f46-34-23-22-199.ngrok-free.app\" -> \"http://localhost:8503\"\n"
          ]
        }
      ],
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# D·ª´ng session c≈© n·∫øu c√≥\n",
        "ngrok.kill()\n",
        "\n",
        "# M·ªü c·ªïng\n",
        "public_url = ngrok.connect(8503, \"http\")\n",
        "print(\"üåê Your Streamlit app is live at:\", public_url)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v-Cr0Z0bUSss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DnyrcA7hMysh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzxVRFnmL3Et",
        "outputId": "146ba082-621b-4752-cc66-c497357d131c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing streamlit_app/app.py\n"
          ]
        }
      ],
      "source": [
        "# %%writefile streamlit_app/app.py\n",
        "# import streamlit as st\n",
        "# from pyspark.sql import SparkSession\n",
        "# from tabs.schema_tab import show_schema\n",
        "# from tabs.duplicate_tab import check_duplicates\n",
        "# from tabs.missing_tab import check_missing\n",
        "\n",
        "# spark = SparkSession.builder.appName(\"Streamlit + Spark\").getOrCreate()\n",
        "\n",
        "# st.title(\"üìä Big Data Dashboard with PySpark\")\n",
        "\n",
        "# uploaded_file = st.file_uploader(\"üìÅ Upload CSV\", type=\"csv\")\n",
        "\n",
        "# if uploaded_file:\n",
        "#     with open(\"streamlit_app/temp.csv\", \"wb\") as f:\n",
        "#         f.write(uploaded_file.getbuffer())\n",
        "\n",
        "#     df = spark.read.csv(\"streamlit_app/temp.csv\", header=True, inferSchema=True)\n",
        "\n",
        "#     tab1, tab2, tab3 = st.tabs([\"üìë Schema\", \"üîÅ Duplicates\", \"üï≥Ô∏è Missing Values\"])\n",
        "\n",
        "#     with tab1:\n",
        "#         show_schema(df)\n",
        "#     with tab2:\n",
        "#         check_duplicates(df)\n",
        "#     with tab3:\n",
        "#         check_missing(df)\n",
        "# else:\n",
        "#     st.info(\"‚¨ÜÔ∏è Upload a CSV to get started.\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}